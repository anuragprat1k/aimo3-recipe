"""
Dataset loading utilities for math reasoning training.

Provides loaders for key math datasets:
- NuminaMath-CoT: ~860k problems with Chain-of-Thought solutions
- NuminaMath-TIR: ~70k problems with Tool-Integrated Reasoning solutions
- OpenMathReasoning: 540k problems with long-reasoning solutions (from NemoSkills)
- MATH: Hendrycks MATH benchmark
"""

from datasets import load_dataset, Dataset, DatasetDict
from typing import Optional


def load_numina_math_cot(
    split: str = "train",
    max_samples: Optional[int] = None,
    filter_source: Optional[str] = None,
) -> Dataset:
    """
    Load NuminaMath-CoT dataset.

    Contains ~860k math problems with Chain-of-Thought solutions from
    Project Numina (AIMO1 winner).

    Args:
        split: Dataset split ("train")
        max_samples: Maximum number of samples to load
        filter_source: Filter by source (e.g., "amc_aime", "olympiad", "cn_k12")

    Returns:
        Dataset with columns: problem, solution, source
    """
    dataset = load_dataset("AI-MO/NuminaMath-CoT", split=split)

    if filter_source:
        dataset = dataset.filter(lambda x: x["source"] == filter_source)

    if max_samples:
        dataset = dataset.select(range(min(max_samples, len(dataset))))

    return dataset


def load_numina_math_tir(
    split: str = "train",
    max_samples: Optional[int] = None,
) -> Dataset:
    """
    Load NuminaMath-TIR dataset.

    Contains ~70k problems with Tool-Integrated Reasoning solutions
    generated by GPT-4 and filtered for correctness.

    Args:
        split: Dataset split ("train")
        max_samples: Maximum number of samples to load

    Returns:
        Dataset with columns: problem, solution (with code blocks)
    """
    dataset = load_dataset("AI-MO/NuminaMath-TIR", split=split)

    if max_samples:
        dataset = dataset.select(range(min(max_samples, len(dataset))))

    return dataset


def load_open_math_reasoning(
    split: str = "train",
    max_samples: Optional[int] = None,
    include_tir: bool = True,
) -> Dataset:
    """
    Load OpenMathReasoning dataset from NemoSkills (AIMO2 winner).

    Contains 540k unique problems with 3.2M long-reasoning solutions,
    plus 1.7M TIR solutions.

    Args:
        split: Dataset split
        max_samples: Maximum samples to load
        include_tir: Include TIR solutions alongside CoT

    Returns:
        Dataset with long-reasoning solutions
    """
    # Main dataset with CoT solutions
    dataset = load_dataset("nvidia/OpenMathReasoning", split=split)

    if max_samples:
        dataset = dataset.select(range(min(max_samples, len(dataset))))

    return dataset


def load_math_dataset(
    split: str = "test",
    level: Optional[str] = None,
    subject: Optional[str] = None,
) -> Dataset:
    """
    Load Hendrycks MATH benchmark dataset.

    Args:
        split: "train" or "test"
        level: Filter by difficulty (e.g., "Level 1" to "Level 5")
        subject: Filter by subject (e.g., "algebra", "geometry", "number_theory")

    Returns:
        Dataset with columns: problem, solution, level, type (subject)
    """
    dataset = load_dataset("lighteval/MATH", split=split)

    if level:
        dataset = dataset.filter(lambda x: x["level"] == level)

    if subject:
        dataset = dataset.filter(lambda x: x["type"] == subject)

    return dataset


def load_hendrycks_math(
    split: str = "train",
    min_level: int = 3,
    max_level: int = 5,
    subjects: Optional[list[str]] = None,
    max_samples: Optional[int] = None,
) -> Dataset:
    """
    Load Hendrycks MATH dataset from EleutherAI with level filtering.

    Loads problems from levels 3-5 (by default) across all subcategories,
    which represent medium to hard difficulty problems.

    Args:
        split: Dataset split ("train" or "test")
        min_level: Minimum difficulty level (1-5), default 3
        max_level: Maximum difficulty level (1-5), default 5
        subjects: List of subjects to include. If None, includes all subjects:
            - algebra
            - counting_and_probability
            - geometry
            - intermediate_algebra
            - number_theory
            - prealgebra
            - precalculus
        max_samples: Maximum number of samples to load (after filtering)

    Returns:
        Dataset with columns: problem, solution, level, type (subject)
    """
    from datasets import concatenate_datasets

    # All available subjects in the Hendrycks MATH dataset
    all_subjects = [
        "algebra",
        "counting_and_probability",
        "geometry",
        "intermediate_algebra",
        "number_theory",
        "prealgebra",
        "precalculus",
    ]

    subjects_to_load = subjects if subjects else all_subjects

    # Valid levels to filter for
    valid_levels = [f"Level {i}" for i in range(min_level, max_level + 1)]

    datasets_list = []

    for subject in subjects_to_load:
        try:
            # Load the specific subject subset
            ds = load_dataset("EleutherAI/hendrycks_math", subject, split=split)

            # Filter by level range
            ds = ds.filter(lambda x: x["level"] in valid_levels)

            # Add subject/type column if not present
            if "type" not in ds.column_names:
                ds = ds.add_column("type", [subject] * len(ds))

            if len(ds) > 0:
                datasets_list.append(ds)
                print(f"  Loaded {len(ds)} problems from {subject} (levels {min_level}-{max_level})")

        except Exception as e:
            print(f"  Warning: Could not load subject '{subject}': {e}")

    if not datasets_list:
        raise ValueError(
            f"No data loaded from EleutherAI/hendrycks_math. "
            f"Check that the dataset is accessible and subjects {subjects_to_load} exist."
        )

    # Concatenate all subject datasets
    combined = concatenate_datasets(datasets_list)

    print(f"  Total: {len(combined)} problems from levels {min_level}-{max_level}")

    if max_samples and len(combined) > max_samples:
        combined = combined.shuffle(seed=42).select(range(max_samples))
        print(f"  Sampled down to {max_samples} problems")

    return combined


def load_amc_aime_dataset(
    source: str = "amc_aime",
    split: str = "train",
) -> Dataset:
    """
    Load AMC/AIME competition problems from NuminaMath.

    Useful for validation during training to prevent overfitting.

    Args:
        source: "amc_aime" for combined
        split: Dataset split

    Returns:
        Dataset with AMC/AIME problems
    """
    dataset = load_dataset("AI-MO/NuminaMath-CoT", split=split)
    dataset = dataset.filter(lambda x: source in x.get("source", ""))
    return dataset


def create_mixed_difficulty_dataset(
    easy_ratio: float = 0.3,
    medium_ratio: float = 0.4,
    hard_ratio: float = 0.3,
    total_samples: int = 100000,
    seed: int = 42,
) -> Dataset:
    """
    Create a mixed-difficulty dataset for curriculum learning.

    Combines problems from different difficulty levels:
    - Easy: MATH Level 1-2, basic problems
    - Medium: MATH Level 3-4, AMC problems
    - Hard: MATH Level 5, AIME, olympiad problems

    Args:
        easy_ratio: Proportion of easy problems
        medium_ratio: Proportion of medium problems
        hard_ratio: Proportion of hard problems
        total_samples: Total number of samples
        seed: Random seed for reproducibility

    Returns:
        Mixed difficulty dataset
    """
    from datasets import concatenate_datasets

    # Load from NuminaMath with different sources
    numina = load_dataset("AI-MO/NuminaMath-CoT", split="train")

    # Categorize by difficulty based on source
    easy_sources = ["cn_k12", "gsm8k"]
    medium_sources = ["math", "amc_aime"]
    hard_sources = ["olympiad", "imo"]

    easy_ds = numina.filter(lambda x: any(s in x.get("source", "") for s in easy_sources))
    medium_ds = numina.filter(lambda x: any(s in x.get("source", "") for s in medium_sources))
    hard_ds = numina.filter(lambda x: any(s in x.get("source", "") for s in hard_sources))

    # Sample according to ratios
    n_easy = int(total_samples * easy_ratio)
    n_medium = int(total_samples * medium_ratio)
    n_hard = int(total_samples * hard_ratio)

    easy_ds = easy_ds.shuffle(seed=seed).select(range(min(n_easy, len(easy_ds))))
    medium_ds = medium_ds.shuffle(seed=seed).select(range(min(n_medium, len(medium_ds))))
    hard_ds = hard_ds.shuffle(seed=seed).select(range(min(n_hard, len(hard_ds))))

    # Combine and shuffle
    combined = concatenate_datasets([easy_ds, medium_ds, hard_ds])
    combined = combined.shuffle(seed=seed)

    return combined


def get_dataset_stats(dataset: Dataset) -> dict:
    """Get statistics about a math dataset."""
    stats = {
        "total_samples": len(dataset),
        "columns": dataset.column_names,
    }

    # Problem length stats
    if "problem" in dataset.column_names:
        problem_lengths = [len(p) for p in dataset["problem"]]
        stats["avg_problem_length"] = sum(problem_lengths) / len(problem_lengths)
        stats["max_problem_length"] = max(problem_lengths)

    # Solution length stats
    if "solution" in dataset.column_names:
        solution_lengths = [len(s) for s in dataset["solution"]]
        stats["avg_solution_length"] = sum(solution_lengths) / len(solution_lengths)
        stats["max_solution_length"] = max(solution_lengths)

    # Source distribution
    if "source" in dataset.column_names:
        from collections import Counter
        stats["source_distribution"] = dict(Counter(dataset["source"]))

    return stats
